{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install environments_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run this, please install [Ollama](https://ollama.com/download) and run\n",
    "`ollama pull` and `ollama pull llama2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments_utils import is_notebook\n",
    "\n",
    "if is_notebook():\n",
    "    # If we always import, we get errors when hosting the .py file\n",
    "\tfrom common_functions import ensure_llama_running, host_chainlit, ensure_installed, get_notebook_name\n",
    "\tensure_installed([{ 'Wikipedia-API': 'wikipediaapi' }, 'langchain', 'chainlit', 'chromadb', \n",
    "\t\t\t\t\t\t{ 'langchain-community': 'langchain_community' }])\n",
    "\tensure_llama_running()\n",
    "\tfilename = get_notebook_name(globals().get('__vsc_ipynb_file__'), 'LLM_Chat_with_Wikipedia_page.ipynb')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import chainlit as cl\n",
    "from IPython.display import display, Markdown\n",
    "from wikipediaapi import Wikipedia\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate  # , PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chains import LLMChain  # , SequentialChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "HOSTING_MODE = True\n",
    "RAG_ENABLED = False\n",
    "\n",
    "CHAT_HISTORY = 'chat_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLM_Chat_with_Wikipedia_page.ipynb to script\n",
      "[NbConvertApp] Writing 6196 bytes to /home/praneeth/Desktop/AI_projects/__pycache__/LLM_Chat_with_Wikipedia_page.ipynb.py\n",
      "Python-dotenv could not parse statement starting at line 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-07 12:08:58 - Loaded .env file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/Desktop/AI_projects/.venv/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.embeddings import OllamaEmbeddings`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-07 12:08:58 - Wikipedia: language=en, user_agent: MyProject (test@example.com) (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
      "2024-04-07 12:08:58 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=Python (programming language)&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
      "2024-04-07 12:08:59 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Python (programming language)&explaintext=1&exsectionformat=wiki\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n",
      "Answer the user's question usi...\n",
      "Functions ready\n",
      "Testing is disabled in hosting mode\n",
      "Chainlit ready\n",
      "2024-04-07 12:09:00 - Your app is available at http://localhost:8000\n",
      "Opening in existing browser session.\n",
      "2024-04-07 12:09:01 - Translation file for en-GB not found. Using default translation en-US.\n",
      "2024-04-07 12:09:01 - Translated markdown file for en-GB not found. Defaulting to chainlit.md.\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "if HOSTING_MODE and is_notebook():\n",
    "\thost_chainlit(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-07 11:21:00 - Wikipedia: language=en, user_agent: MyProject (test@example.com) (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
      "2024-04-07 11:21:00 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=Python (programming language)&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
      "2024-04-07 11:21:01 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Python (programming language)&explaintext=1&exsectionformat=wiki\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n"
     ]
    }
   ],
   "source": [
    "wikipedia = Wikipedia('MyProject (test@example.com)', 'en')\n",
    "\n",
    "def get_wikipedia_page(page_name):\n",
    "\tpage = wikipedia.page(page_name)\n",
    "\t\n",
    "\tif page.exists() and page.text:\n",
    "\t\treturn page.text\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "page_content = get_wikipedia_page('Python (programming language)')\n",
    "if page_content is None:\n",
    "\traise ValueError('Page not found')\n",
    "print(page_content[:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model='llama2')\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user's question usi...\n"
     ]
    }
   ],
   "source": [
    "config = RunnableConfig(\n",
    "\tmax_tokens=50,\n",
    "\ttemperature=0.5,\n",
    "\ttop_p=0.9,\n",
    "\ttop_k=0,\n",
    "\tnum_return_sequences=1,\n",
    "\tmax_length=100,\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# prompt can also be saved to a file and used as a template\n",
    "prompt = '''\n",
    "Answer the user's question using the text below.\n",
    "Avoid words like 'According to'.\n",
    "Keep the answers very short and to the point upto 25 words max.\n",
    "# emphasizing on a short answer for a faster response and saving CPU time\n",
    "'''\n",
    "\n",
    "# remove comments and clean up the prompt to reduce tokens\n",
    "prompt = re.sub(r'#.*', '', prompt)  # remove comments\n",
    "prompt = re.sub(r'\\n+', '\\n', prompt)  # remove extra newlines where there are more than one\n",
    "prompt = '\\n'.join([line.strip() for line in prompt.split('\\n')])  # strip each line\n",
    "prompt = prompt.strip()\n",
    "# prompt = PromptTemplate(template=prompt, input_variables=[])  # create a prompt template\n",
    "\n",
    "print(prompt[:30] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(memory=ConversationBufferMemory(input_key='question', memory_key='chat_history'), prompt=ChatPromptTemplate(input_variables=['query_context', 'question'], messages=[SystemMessage(content=\"Answer the user's question using the text below.\\nAvoid words like 'According to'.\\nKeep the answers very short and to the point upto 25 words max.\"), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query_context'], template='{query_context}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=Ollama(), output_key='query_answer')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=prompt),\n",
    "\t('system', '{query_context}'),  # context from the wikipedia relevant to the query\n",
    "\t('user', '{question}'),\n",
    "])\n",
    "\n",
    "# query_memory = ConversationBufferMemory(input_key=\"question\", memory_key=CHAT_HISTORY)\n",
    "\n",
    "# chain = promptTemplate | llm | StrOutputParser()  # Parse output as string\n",
    "# Different operations are chained together to form a 'pipeline'.\n",
    "# The output of one operation is passed as input to the next.\n",
    "\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=chatPromptTemplate,\n",
    "\tverbose=False,\n",
    "\toutput_key=\"query_answer\",\n",
    "\t# memory=query_memory,\n",
    ")\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing vector databases and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = None\n",
    "\n",
    "def setup_chroma_db():\n",
    "\ttemp_path = os.path.join('__pycache__', 'wikipedia_temp.txt')\n",
    "\twith open(temp_path, 'w') as f:\n",
    "\t\tf.write(page_content)\n",
    "\n",
    "\traw_documents = TextLoader(temp_path).load()\n",
    "\ttext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\tdocuments = text_splitter.split_documents(raw_documents)\n",
    "\tdb = Chroma.from_documents(documents, embedding=OllamaEmbeddings())\n",
    "\treturn db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "CHARACTER_LIMIT = 5000\n",
    "\n",
    "def answer_question_RAG(question):\n",
    "\tif db is None:\n",
    "\t\tsetup_chroma_db()\n",
    "\tdocs = db.similarity_search(question)\n",
    "\tif not docs or not len(docs):\n",
    "\t\treturn None\n",
    "\tquery_context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\t# print(f'Query context: {query_context[:1000]}...')\n",
    "\tanswer = chain.invoke(\n",
    "\t\t{ \"question\": question, \"query_context\": query_context },\n",
    "\t\tconfig=config,\n",
    "\t\toutput_key=\"query_answer\",\n",
    "\t)\n",
    "\treturn answer[\"query_answer\"]\n",
    "\n",
    "def answer_question(question):\n",
    "\tif RAG_ENABLED:\n",
    "\t\treturn answer_question_RAG(question)\n",
    "\tanswer = chain.invoke(\n",
    "\t\t{ \"question\": question, \"query_context\": page_content[:CHARACTER_LIMIT] },\n",
    "\t\tconfig=config,\n",
    "\t)\n",
    "\treturn answer[\"query_answer\"]\n",
    "\n",
    "def test_for_question(question):\n",
    "\tprint(f'Question: {question}')\n",
    "\tanswer = answer_question(question)\n",
    "\tanswer = f'Answer: {answer}'\n",
    "\tdisplay(Markdown(answer))\n",
    "\ttime.sleep(2)  # CPU cooldown break\n",
    "\t# return answer\n",
    "\n",
    "print('Functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with some queries (disabled in hosting mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who invented python\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Guido van Rossum invented Python in the late 1980s."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\tprint('Testing is disabled in hosting mode')\n",
    "else:\n",
    "\tquestions_to_test = [\n",
    "\t\t# 'what is python? explain short in simple words',\n",
    "\t\t# 'why python? why not javascript?',\n",
    "\t\t# 'what is garbage collector in java?',  # Unrelated question\n",
    "\t\t'who invented python',\n",
    "\t\t# 'quien inventó python',  # Asking in Spanish - who invented python\n",
    "\t\t# 'पाइथॉन का आविष्कार किसने किया',  # same in Hindi\n",
    "\t]\n",
    "\tfor question in questions_to_test:\n",
    "\t\ttest_for_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosting with Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainlit ready\n"
     ]
    }
   ],
   "source": [
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "\tcl.user_session.set(CHAT_HISTORY, [])\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "\tanswer = answer_question(message.content)\n",
    "\tresult = cl.Message(content=answer)\n",
    "\n",
    "\tchat_history = cl.user_session.get(CHAT_HISTORY)\n",
    "\tchat_history.append(HumanMessage(content=message.content))\n",
    "\tchat_history.append(AIMessage(content=answer))\n",
    "\tcl.user_session.set(CHAT_HISTORY, chat_history)\n",
    "\n",
    "\tawait result.send()\n",
    "\n",
    "print('Chainlit ready')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
