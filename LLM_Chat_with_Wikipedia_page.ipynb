{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install environments_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments_utils import is_notebook\n",
    "\n",
    "if is_notebook():\n",
    "    # If we always import, we get errors when hosting the .py file\n",
    "\tfrom common_functions import ensure_llama_running, host_chainlit, ensure_installed, get_notebook_name\n",
    "\tensure_installed([{ 'Wikipedia-API': 'wikipediaapi' }, 'langchain', 'chainlit', 'chromadb', \n",
    "\t\t\t\t\t\t'langchain_community', 'transformers'])\n",
    "\tensure_llama_running()\n",
    "\tfilename = get_notebook_name(globals().get('__vsc_ipynb_file__'), 'LLM_Chat_with_Wikipedia_page.ipynb')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import chainlit as cl\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from wikipediaapi import Wikipedia\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate  # , PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chains import LLMChain  # , SequentialChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "HOSTING_MODE = False\n",
    "RAG_ENABLED = False\n",
    "\n",
    "CHAT_HISTORY = 'chat_history'\n",
    "vector_db = None\n",
    "load_dotenv()\n",
    "llm_model = os.getenv('LLM_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOSTING_MODE and is_notebook():\n",
    "\thost_chainlit(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 07:48:47 - Wikipedia: language=en, user_agent: MyProject (test@example.com) (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n"
     ]
    }
   ],
   "source": [
    "wikipedia = Wikipedia('MyProject (test@example.com)', 'en')\n",
    "\n",
    "def get_wikipedia_page(page_name):\n",
    "\tcache_file = os.path.join('__pycache__', f'wiki_{page_name}.txt')\n",
    "\tif os.path.exists(cache_file):\n",
    "\t\twith open(cache_file, 'r') as f:\n",
    "\t\t\treturn f.read()\n",
    "\n",
    "\tpage = wikipedia.page(page_name)\n",
    "\t\n",
    "\tif page.exists() and page.text:\n",
    "\t\twith open(cache_file, 'w') as f:\n",
    "\t\t\tf.write(page.text)\n",
    "\t\treturn page.text\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "page_content = get_wikipedia_page('Python (programming language)')\n",
    "if page_content is None:\n",
    "\traise ValueError('Page not found')\n",
    "print(page_content[:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=llm_model)\n",
    "\n",
    "config = RunnableConfig(\n",
    "\t# max_tokens=35,\n",
    "\t# temperature=0.5,\n",
    "\t# top_p=0.9,\n",
    "\t# top_k=0,\n",
    "\t# num_return_sequences=1,\n",
    "\t# max_length=100,\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial tokens: 39\n",
      "Number of tokens after cleanup: 31\n"
     ]
    }
   ],
   "source": [
    "# prompt can also be saved to a file and used as a template\n",
    "prompt = '''\n",
    "Answer the user's question using the text below.\n",
    "Avoid words like 'According to', 'Sure'.\n",
    "Keep the answers very short and to the point upto 25 words max.\n",
    "# emphasizing on a short answer for a faster response and saving CPU time\n",
    "'''\n",
    "\n",
    "# remove comments and clean up the prompt to reduce tokens\n",
    "prompt = re.sub(r'#.*', '', prompt)  # remove comments\n",
    "\n",
    "print('Number of initial tokens:', llm.get_num_tokens(prompt))\n",
    "\n",
    "prompt = re.sub(r'\\n+', '\\n', prompt)  # remove extra newlines where there are more than one\n",
    "prompt = '\\n'.join([line.strip() for line in prompt.split('\\n')])  # strip each line\n",
    "prompt = prompt.strip()\n",
    "# remove punctuations at the start and end of the prompt\n",
    "punctuations = ',.!?'\n",
    "while prompt[0] in punctuations:\n",
    "\tprompt = prompt[1:]\n",
    "while prompt[-1] in punctuations:\n",
    "\tprompt = prompt[:-1]\n",
    "prompt = prompt.replace('\\'s', 's')  # replace 's with s to save token usage for '\n",
    "for article in ['a', 'an', 'the']:  # remove 'a ', 'an ', 'the '\n",
    "\tprompt = prompt.replace(article + ' ', '')\n",
    "\tprompt = prompt.replace(article.capitalize() + ' ', '')\n",
    "\n",
    "# Print number of tokens in the prompt\n",
    "print('Number of tokens after cleanup:', llm.get_num_tokens(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=ChatPromptTemplate(input_variables=['query_context', 'question'], messages=[SystemMessage(content=\"Answer users question using text below.\\nAvoid words like 'According to', 'Sure'.\\nKeep answers very short and to point upto 25 words max\"), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query_context'], template='{query_context}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=Ollama(model='stablelm2'), output_key='query_answer')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# promptTemplate = PromptTemplate(template=prompt, input_variables=[])  # create a prompt template\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(prompt),\n",
    "\t('system', '{query_context}'),  # context from the wikipedia relevant to the query\n",
    "\t('user', '{question}'),\n",
    "])\n",
    "\n",
    "# query_memory = ConversationBufferMemory(input_key='question', memory_key=CHAT_HISTORY)\n",
    "\n",
    "# chain = promptTemplate | llm | StrOutputParser()  # Parse output as string\n",
    "# Different operations are chained together to form a 'pipeline'.\n",
    "# The output of one operation is passed as input to the next.\n",
    "\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=chatPromptTemplate,\n",
    "\tverbose=False,\n",
    "\toutput_key='query_answer',\n",
    "\t# memory=query_memory,\n",
    ")\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing vector databases and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chroma_db():\n",
    "\tglobal vector_db\n",
    "\tif vector_db:\n",
    "\t\treturn vector_db\n",
    "\tprint('Setting up Chroma DB...')\n",
    "\t# temp_path = os.path.join('__pycache__', 'wikipedia_temp.txt')\n",
    "\t# with open(temp_path, 'w') as f:\n",
    "\t# \tf.write(page_content)\n",
    "\t# raw_documents = TextLoader(temp_path).load()\n",
    "\t# os.remove(temp_path)\n",
    "\traw_documents = [Document(page_content=page_content)]\n",
    "\ttext_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "\tdocuments = text_splitter.split_documents(raw_documents)\n",
    "\tvector_db = Chroma.from_documents(documents, embedding=OllamaEmbeddings())\n",
    "\tprint('Finished Chroma DB setup.')\n",
    "\treturn vector_db\n",
    "\n",
    "if RAG_ENABLED and not vector_db:\n",
    "\tsetup_chroma_db()\n",
    "\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "CHARACTER_LIMIT = 5000\n",
    "rag_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def search_vectorDB(question, k=2):\n",
    "\tsetup_chroma_db()\n",
    "\tdocs = vector_db.similarity_search(question, k=k)\n",
    "\treturn docs\n",
    "\t# query_context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\t# return query_context\n",
    "\n",
    "def answer_question(question):\n",
    "\tif RAG_ENABLED:\n",
    "\t\tsearch_docs = search_vectorDB(question)\n",
    "\t\tresponse = rag_chain.run(input_documents=search_docs, question=question)\n",
    "\t\treturn response\n",
    "\t\t# response = rag_chain.invoke(\n",
    "\t\t# \t{\n",
    "\t\t# \t\t\"input_documents\": search_docs,\n",
    "\t\t# \t\t\"question\": question,\n",
    "\t\t# \t},\n",
    "\t\t# \tconfig=config,\n",
    "\t\t# \toutput_key=\"query_answer\",\n",
    "\t\t# )\n",
    "\t\t# return response['output_text']\n",
    "\telse:\n",
    "\t\tquery_context = page_content[:CHARACTER_LIMIT]\n",
    "\t\tanswer = chain.invoke(\n",
    "\t\t\t{\"question\": question, \"query_context\": query_context},\n",
    "\t\t\tconfig=config,\n",
    "\t\t\toutput_key=\"query_answer\",\n",
    "\t\t)\n",
    "\treturn answer[\"query_answer\"]\n",
    "\n",
    "def test_for_question(question):\n",
    "\tprint(f'Question: {question}')\n",
    "\tanswer = answer_question(question)\n",
    "\tanswer = f'Answer: {answer}'\n",
    "\tdisplay(Markdown(answer))\n",
    "\ttime.sleep(2)  # CPU cooldown break\n",
    "\t# return answer\n",
    "\n",
    "print('Functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with some queries (disabled in hosting mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who invented python\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Guido van Rossum invented Python in the late 1.. Early releases included versions 0.9.0, 2.0, 3.0, and others. Python has undergone several major revisions since its inception in 1..\n",
       "Python consistently ranks as one of the most popular programming languages due to its wide range of features and capabilities. Its dynamic typing system and garbage collection make it easy to develop efficient software applications while allowing developers to write shorter code using familiar syntax. Additionally, Python's comprehensive standard library provides a broad set of functionalities that can be used throughout an application without needing additional libraries or modules."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\tprint('Testing is disabled in hosting mode')\n",
    "else:\n",
    "\tquestions_to_test = [\n",
    "\t\t# 'what is python? explain short in simple words',\n",
    "\t\t# 'why python? why not javascript?',\n",
    "\t\t# 'what is garbage collector in java?',  # Unrelated question\n",
    "\t\t'who invented python',\n",
    "\t\t# 'quien inventó python',  # Asking in Spanish - who invented python\n",
    "\t\t# 'पाइथॉन का आविष्कार किसने किया',  # same in Hindi\n",
    "\t]\n",
    "\tfor question in questions_to_test:\n",
    "\t\ttest_for_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosting with Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available only in hosting mode\n"
     ]
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\t@cl.on_chat_start\n",
    "\tasync def on_chat_start():\n",
    "\t\tcl.user_session.set(CHAT_HISTORY, [])\n",
    "\n",
    "\t@cl.on_message\n",
    "\tasync def on_message(message: cl.Message):\n",
    "\t\tanswer = answer_question(message.content)\n",
    "\t\tresult = cl.Message(content=answer)\n",
    "\n",
    "\t\tchat_history = cl.user_session.get(CHAT_HISTORY)\n",
    "\t\tchat_history.append(HumanMessage(content=message.content))\n",
    "\t\tchat_history.append(AIMessage(content=answer))\n",
    "\t\tcl.user_session.set(CHAT_HISTORY, chat_history)\n",
    "\n",
    "\t\tawait result.send()\n",
    "\n",
    "\tprint('Chainlit ready')\n",
    "\n",
    "else:\n",
    "    print('Available only in hosting mode')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
