{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install environments_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run this, please install [Ollama](https://ollama.com/download) and run\n",
    "`ollama pull` and `ollama pull llama2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:03:30 - Loaded .env file\n",
      "2024-04-06 20:03:30 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:03:30 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:03:30 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:03:30 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:03:30 - Python-dotenv could not parse statement starting at line 2\n"
     ]
    }
   ],
   "source": [
    "from environments_utils import is_notebook\n",
    "\n",
    "if is_notebook():\n",
    "    # If we always import, we get errors when hosting the .py file\n",
    "\tfrom common_functions import ensure_llama_running, host_chainlit, ensure_installed, get_notebook_name\n",
    "\tensure_installed([{ 'Wikipedia-API': 'wikipediaapi' }, 'langchain', 'chainlit', 'chromadb', \n",
    "\t\t\t\t\t\t{ 'langchain-community': 'langchain_community' }])\n",
    "\tensure_llama_running()\n",
    "\tfilename = get_notebook_name(globals().get('__vsc_ipynb_file__'), 'LLM_Chat_with_Wikipedia_page.ipynb')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "from wikipediaapi import Wikipedia\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain  # , SequentialChain\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import chainlit as cl\n",
    "\n",
    "HOSTING_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLM_Chat_with_Wikipedia_page.ipynb to script\n",
      "[NbConvertApp] Writing 5546 bytes to __pycache__/LLM_Chat_with_Wikipedia_page.ipynb.py\n",
      "Python-dotenv could not parse statement starting at line 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:04:36 - Loaded .env file\n",
      "2024-04-06 20:04:37 - Wikipedia: language=en, user_agent: MyProject (test@example.com) (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
      "2024-04-06 20:04:37 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=Python (programming language)&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
      "2024-04-06 20:04:37 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Python (programming language)&explaintext=1&exsectionformat=wiki\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n",
      "Answer the user's question using the text below.\n",
      "A...\n",
      "Functions ready\n",
      "2024-04-06 20:04:38 - Created a chunk of size 3435, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 4161, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 2935, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 4798, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1199, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 2799, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1044, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1085, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1493, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1187, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 2149, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 4077, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Created a chunk of size 1415, which is longer than the specified 1000\n",
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/Desktop/AI_projects/.venv/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.embeddings import OllamaEmbeddings`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:04:38 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 20:04:38 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Testing is disabled in hosting mode\n",
      "Chainlit ready\n",
      "2024-04-06 20:05:22 - Your app is available at http://localhost:8000\n",
      "Opening in existing browser session.\n",
      "2024-04-06 20:05:23 - Translation file for en-GB not found. Using default translation en-US.\n",
      "2024-04-06 20:05:23 - Translated markdown file for en-GB not found. Defaulting to chainlit.md.\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "if HOSTING_MODE and is_notebook():\n",
    "\thost_chainlit(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-06 19:56:32 - Wikipedia: language=en, user_agent: MyProject (test@example.com) (Wikipedia-API/0.6.0; https://github.com/martin-majlis/Wikipedia-API/), extract_format=ExtractFormat.WIKI\n",
      "2024-04-06 19:56:32 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=info&titles=Python (programming language)&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle\n",
      "2024-04-06 19:56:33 - Request URL: https://en.wikipedia.org/w/api.php?action=query&prop=extracts&titles=Python (programming language)&explaintext=1&exsectionformat=wiki\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n"
     ]
    }
   ],
   "source": [
    "wikipedia = Wikipedia('MyProject (test@example.com)', 'en')\n",
    "\n",
    "def get_wikipedia_page(page_name):\n",
    "\tpage = wikipedia.page(page_name)\n",
    "\t\n",
    "\tif page.exists() and page.text:\n",
    "\t\treturn page.text\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "page_content = get_wikipedia_page('Python (programming language)')\n",
    "if page_content is None:\n",
    "\traise ValueError('Page not found')\n",
    "print(page_content[:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model='llama2')\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user's question using the text below.\n",
      "A...\n"
     ]
    }
   ],
   "source": [
    "# prompt can also be saved to a file and used as a template\n",
    "prompt = '''\n",
    "Answer the user's question using the text below.\n",
    "Avoid words like 'Hi', 'According to', 'In my opinion', etc.\n",
    "Keep the answers very short and to the point upto 25 words max.\n",
    "# emphasizing on a short answer for a faster response and saving CPU time\n",
    "'''\n",
    "\n",
    "# remove comments and clean up the prompt to reduce tokens\n",
    "prompt = re.sub(r'#.*', '', prompt)\n",
    "prompt = re.sub(r'\\n+', '\\n', prompt)\n",
    "prompt = prompt.strip()\n",
    "print(prompt[:50] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(\n",
    "\tmax_tokens=50,\n",
    "\ttemperature=0.5,\n",
    "\ttop_p=0.9,\n",
    "\ttop_k=0,\n",
    "\tnum_return_sequences=1,\n",
    "\tmax_length=100,\n",
    ")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "promptTemplate = ChatPromptTemplate.from_messages([\n",
    "\t('system', prompt),\n",
    "\t('system', '{query_context}'),  # context from the wikipedia relevant to the query\n",
    "\t('user', '{question}'),\n",
    "])\n",
    "\n",
    "# chain = promptTemplate | llm | StrOutputParser()  # Parse output as string\n",
    "# # Different operations are chained together to form a 'pipeline'.\n",
    "# # The output of one operation is passed as input to the next.\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=promptTemplate,\n",
    "\tverbose=False,\n",
    "\toutput_key=\"query_answer\",\n",
    ")\n",
    "\n",
    "CHARACTER_LIMIT = 5000\n",
    "\n",
    "def answer_question(question):\n",
    "\tanswer = chain.invoke(\n",
    "\t\t{ \"question\": question, \"query_context\": page_content[:CHARACTER_LIMIT] },\n",
    "\t\tconfig=config,\n",
    "\t)\n",
    "\treturn answer[\"query_answer\"]\n",
    "\n",
    "\n",
    "def test_for_question(question):\n",
    "\tprint(f'Question: {question}')\n",
    "\tanswer = answer_question(question)\n",
    "\tanswer = f'Answer: {answer}'\n",
    "\tdisplay(Markdown(answer))\n",
    "\ttime.sleep(2)  # CPU cooldown break\n",
    "\t# return answer\n",
    "\n",
    "print('Functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing vector databases and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-06 19:56:33 - Created a chunk of size 3435, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 4161, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 2935, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 4798, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1199, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 2799, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1044, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1085, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1493, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1187, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 2149, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 4077, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Created a chunk of size 1415, which is longer than the specified 1000\n",
      "2024-04-06 19:56:33 - Python-dotenv could not parse statement starting at line 2\n",
      "2024-04-06 19:56:33 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7ee46c532ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "temp_path = os.path.join('__pycache__', 'wikipedia_temp.txt')\n",
    "with open(temp_path, 'w') as f:\n",
    "\tf.write(page_content)\n",
    "\n",
    "raw_documents = TextLoader(temp_path).load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, embedding=OllamaEmbeddings())\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "\tdocs = db.similarity_search(question)\n",
    "\tif not docs or not len(docs):\n",
    "\t\treturn None\n",
    "\tquery_context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\t# print(f'Query context: {query_context[:1000]}...')\n",
    "\tanswer = chain.invoke(\n",
    "\t\t{ \"question\": question, \"query_context\": query_context },\n",
    "\t\tconfig=config,\n",
    "\t)\n",
    "\treturn answer[\"query_answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with some queries (disabled in hosting mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing is disabled in hosting mode\n"
     ]
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\tprint('Testing is disabled in hosting mode')\n",
    "else:\n",
    "\tquestions_to_test = [\n",
    "\t\t# 'what is python? explain short in simple words',\n",
    "\t\t# 'why python? why not javascript?',\n",
    "\t\t# 'what is garbage collector in java?',  # Unrelated question\n",
    "\t\t'who invented python',\n",
    "\t\t# 'quien inventó python',  # Asking in Spanish - who invented python\n",
    "\t\t# 'पाइथॉन का आविष्कार किसने किया',  # same in Hindi\n",
    "\t]\n",
    "\tfor question in questions_to_test:\n",
    "\t\ttest_for_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosting with Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainlit ready\n"
     ]
    }
   ],
   "source": [
    "# @cl.on_chat_start\n",
    "# async def on_chat_start():\n",
    "# \tchain = promptTemplate | llm | StrOutputParser()\n",
    "# \tcl.user_session.set('runnable', chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.Message):\n",
    "\t# runnable = cl.user_session.get('runnable')  ## type: Runnable\n",
    "\tresult = cl.Message(content='')\n",
    "\n",
    "\tasync for chunk in chain.astream(\n",
    "\t\t{ 'question': message.content },\n",
    "\t\tconfig=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "\t):\n",
    "\t\tawait result.stream_token(chunk['query_answer'])\n",
    "\n",
    "\tawait result.send()\n",
    "\n",
    "print('Chainlit ready')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
