{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "WzpK-j2uZrct",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzpK-j2uZrct",
        "outputId": "f269cb9a-18be-49f5-db57-7ec0420a30b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print('Completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db677693",
      "metadata": {
        "id": "db677693"
      },
      "source": [
        "Set a device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "89078732",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "89078732",
        "outputId": "9004bb04-b815-4f4f-c77f-afcd3e56048e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "seed = 42\n",
        "\n",
        "if torch.accelerator.is_available():\n",
        "\tdevice = torch.accelerator.current_accelerator().type\n",
        "\ttorch.set_default_device(device)\n",
        "\tdef set_seed(seed=seed):\n",
        "\t\ttorch.cuda.manual_seed_all(seed)\n",
        "\tset_seed()\n",
        "else:\n",
        "\tdevice = 'cpu'\n",
        "\tdef set_seed(seed=seed):\n",
        "\t\ttorch.manual_seed(seed)\n",
        "\tset_seed()\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011f35d7",
      "metadata": {
        "id": "011f35d7"
      },
      "source": [
        "## Loading a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59beb7ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59beb7ae",
        "outputId": "dd976797-a4f5-4ac6-b43c-808c7a95b258"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "\t'bert-base-uncased',  # Use something like BERT for classification\n",
        "\tnum_labels=2,  # Binary classification - positive/negative\n",
        ").to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# # 'google/gemma-3-4b-it' 'meta-llama/Llama-3.2-3B-Instruct', 'meta-llama/Llama-3.2-11B-Vision-Instruct', 'bert-base-uncased', 'gpt2'\n",
        "# inputs = tokenizer(['User: Hi! Assistant:'], return_tensors='pt').to(device)\n",
        "# inputs['input_ids'][:5]  # test the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5a526d",
      "metadata": {
        "id": "2c5a526d"
      },
      "source": [
        "## Load a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e2a3b70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a3b70e",
        "outputId": "2a43a625-0284-4c89-84a9-2fd7a98bb8ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 20000 training samples & 5000 test samples\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "dataset = load_dataset('imdb')  # Load IMDB dataset\n",
        "# split into train/test\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=seed)\n",
        "print(f'Dataset loaded: {len(dataset[\"train\"])} training samples & {len(dataset[\"test\"])} test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bb4e749b",
      "metadata": {
        "id": "bb4e749b"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "\tdef __init__(self, data, tokenizer, max_length=100):\n",
        "\t\tself.data = data\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tself.max_length = max_length\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\ttext = self.data[idx]['text']\n",
        "\t\tlabel = self.data[idx]['label']\n",
        "\t\tencoding = self.tokenizer(text, truncation=True, padding='max_length',\n",
        "\t\t\t max_length=self.max_length, return_tensors='pt').to(device)\n",
        "\t\treturn {\n",
        "\t\t\t'input_ids': encoding['input_ids'].squeeze(0),\n",
        "\t\t\t'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "\t\t\t'label': torch.tensor(label)\n",
        "\t\t}\n",
        "\n",
        "batch_size = 32\n",
        "generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "train_data = IMDBDataset(dataset['train'], tokenizer)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=generator)\n",
        "\n",
        "test_data = IMDBDataset(dataset['test'], tokenizer)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=True, generator=generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e430ba",
      "metadata": {
        "id": "c1e430ba"
      },
      "source": [
        "## Create a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8554e0d5",
      "metadata": {
        "id": "8554e0d5"
      },
      "outputs": [],
      "source": [
        "# create a 3-layered neural network with ReLU activation\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# model = nn.Sequential(\n",
        "# \tnn.Linear(input_dim, hidden_dim),\n",
        "# \tnn.ReLU(),\n",
        "# \tnn.Linear(hidden_dim, output_dim),\n",
        "# \tnn.Softmax(dim=1)\n",
        "# ).to(device)\n",
        "class Classifier(nn.Module):\n",
        "\tdef __init__(self, model, hidden_dim=768, output_dim=2, dropout_rate=0.1):\n",
        "\t\tsuper(Classifier, self).__init__()\n",
        "\t\tself.model = model\n",
        "\t\tfor param in self.model.parameters():\n",
        "\t\t\t# # Freeze BERT parameters to speed up training (optional)\n",
        "\t\t\t# param.requires_grad = False\n",
        "\t\t\tparam.requires_grad = True\n",
        "\n",
        "\t\t# self.dropout = nn.Dropout(dropout_rate)\n",
        "\t\t# self.classifier = nn.Sequential(\n",
        "\t\t# \tnn.Linear(hidden_dim, hidden_dim // 2),\n",
        "\t\t# \tnn.ReLU(),\n",
        "\t\t# \tnn.Dropout(dropout_rate),\n",
        "\t\t# \tnn.Linear(hidden_dim // 2, output_dim)\n",
        "\t\t# )\n",
        "\n",
        "\tdef forward(self, input_ids, attention_mask):\n",
        "\t\t# Get BERT embeddings - just use the [CLS] token representation\n",
        "\t\toutputs = self.model(input_ids, attention_mask)\n",
        "\t\t# with torch.no_grad():\n",
        "\t\t# \toutputs = self.model(input_ids, attention_mask)\n",
        "\t\t# \tpooled_output = outputs.last_hidden_state[:, 0, :]  # Get the [CLS] token representation\n",
        "\n",
        "\t\t# # Apply dropout and classification\n",
        "\t\t# x = self.dropout(pooled_output)\n",
        "\t\t# logits = self.classifier(x)\n",
        "\n",
        "\t\tlogits = outputs.logits\n",
        "\t\treturn logits\n",
        "\n",
        "model = Classifier(model).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "\tmodel.parameters(),  # [p for p in model.parameters() if p.requires_grad],\n",
        "\tlr=2e-5,\n",
        "\tweight_decay=0.01\n",
        ")\n",
        "num_epochs = 5  # Increase number of epochs for better performance\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14ea806",
      "metadata": {
        "id": "c14ea806"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b881d9f",
      "metadata": {
        "id": "0b881d9f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "\tset_seed()\n",
        "\tmodel.train()\n",
        "\tepoch_loss = 0\n",
        "\tall_preds = []\n",
        "\tall_labels = []\n",
        "\n",
        "\tfor batch in dataloader:\n",
        "\t\tinput_ids = batch['input_ids'].to(device)\n",
        "\t\tattention_mask = batch['attention_mask'].to(device)\n",
        "\t\tlabels = batch['label'].to(device)\n",
        "\n",
        "\t\t# Forward pass\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\toutputs = model(input_ids, attention_mask)\n",
        "\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\t\t# Backward pass\n",
        "\t\tloss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\t# Record metrics\n",
        "\t\tepoch_loss += loss.item()\n",
        "\t\tpreds = torch.argmax(outputs, dim=1).cpu().numpy() # Can be done on only a CPU\n",
        "\t\tall_preds.extend(preds)\n",
        "\t\tall_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\t# Calculate accuracy\n",
        "\taccuracy = accuracy_score(all_labels, all_preds)\n",
        "\treturn epoch_loss / len(dataloader), accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "\tset_seed()\n",
        "\tmodel.eval()  # set model to evaluation mode\n",
        "\tepoch_loss = 0\n",
        "\tall_preds = []\n",
        "\tall_labels = []\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch in dataloader:\n",
        "\t\t\tinput_ids = batch['input_ids'].to(device)\n",
        "\t\t\tattention_mask = batch['attention_mask'].to(device)\n",
        "\t\t\tlabels = batch['label'].to(device)\n",
        "\n",
        "\t\t\t# Forward pass\n",
        "\t\t\toutputs = model(input_ids, attention_mask)\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\t\t\tepoch_loss += loss.item()\n",
        "\t\t\tpreds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\t\t\tall_preds.extend(preds)\n",
        "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\taccuracy = accuracy_score(all_labels, all_preds)\n",
        "\treturn epoch_loss / len(dataloader), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "87e980be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87e980be",
        "outputId": "ca391c47-67c7-49a0-ee78-0c81577db4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            ".\n",
            ".\n",
            "Epoch 1/5:\n",
            "  Train Loss: 0.7100, Train Accuracy: 0.4837\n",
            "  Val Loss: 0.7080, Val Accuracy: 0.4936\n",
            ".\n",
            ".\n",
            "Epoch 2/5:\n",
            "  Train Loss: 0.7040, Train Accuracy: 0.4959\n",
            "  Val Loss: 0.6973, Val Accuracy: 0.5084\n",
            ".\n",
            ".\n",
            "Epoch 3/5:\n",
            "  Train Loss: 0.6921, Train Accuracy: 0.5284\n",
            "  Val Loss: 0.6798, Val Accuracy: 0.5820\n",
            ".\n",
            ".\n",
            "Epoch 4/5:\n",
            "  Train Loss: 0.6713, Train Accuracy: 0.5965\n",
            "  Val Loss: 0.6425, Val Accuracy: 0.6638\n",
            ".\n",
            ".\n",
            "Epoch 5/5:\n",
            "  Train Loss: 0.6158, Train Accuracy: 0.6893\n",
            "  Val Loss: 0.5680, Val Accuracy: 0.7478\n",
            "Loaded best model with validation accuracy: 0.7478, Loss: 0.5680\n"
          ]
        }
      ],
      "source": [
        "best_accuracy = 0\n",
        "best_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "print('Training...')\n",
        "for epoch in range(num_epochs):\n",
        "\ttrain_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\tprint('.')\n",
        "\tval_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "\tprint('.')\n",
        "\n",
        "\tscheduler.step()  # Update learning rate\n",
        "\n",
        "\tif val_acc > best_accuracy:  # Save best model\n",
        "\t\tbest_accuracy = val_acc\n",
        "\t\tbest_loss = val_loss\n",
        "\t\tbest_model_state = model.state_dict().copy()\n",
        "\n",
        "\tprint(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "\tprint(f'  Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "\tprint(f'  Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "# Load best model and evaluation results\n",
        "if best_model_state:\n",
        "\tmodel.load_state_dict(best_model_state)\n",
        "\tprint(f'Loaded best model with validation accuracy: {best_accuracy:.4f}, Loss: {best_loss:.4f}')\n",
        "\n",
        "\t# # Final Evaluation - on the full test set\n",
        "\t# final_loss, final_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "\t# print(f'Final Test Results:')\n",
        "\t# print(f'  Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713bf967",
      "metadata": {
        "id": "713bf967"
      },
      "source": [
        "## Predict on custom text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a3bbd589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bbd589",
        "outputId": "8ca85fed-72d9-46a3-a4d9-d92b706978dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing with example reviews:\n",
            "Review: This movie was fantastic! I loved it.\n",
            "Predicted sentiment: Positive\n",
            "\n",
            "Review: The acting was terrible and the plot made no sense.\n",
            "Predicted sentiment: Negative\n",
            "\n",
            "Review: A masterpiece of cinema that will be remembered for decades.\n",
            "Predicted sentiment: Positive\n",
            "\n",
            "Review: I wasted two hours of my life watching this garbage.\n",
            "Predicted sentiment: Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "\t# Tokenize input text\n",
        "\tencoded_text = tokenizer(text, truncation=True,\n",
        "        padding='max_length', max_length=512, return_tensors='pt')\n",
        "\n",
        "\tinput_ids = encoded_text['input_ids'].to(device)\n",
        "\tattention_mask = encoded_text['attention_mask'].to(device)\n",
        "\n",
        "\t# Make prediction\n",
        "\tmodel.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\toutputs = model(input_ids, attention_mask)\n",
        "\t\tprediction = torch.argmax(outputs, dim=1).item()\n",
        "\n",
        "\tsentiment = 'Positive' if prediction == 1 else 'Negative'\n",
        "\treturn sentiment\n",
        "\n",
        "\n",
        "# Test with some example reviews\n",
        "test_reviews = [\n",
        "\t'This movie was fantastic! I loved it.',\n",
        "\t'The acting was terrible and the plot made no sense.',\n",
        "\t'A masterpiece of cinema that will be remembered for decades.',\n",
        "\t'I wasted two hours of my life watching this garbage.',\n",
        "]\n",
        "\n",
        "print('\\nTesting with example reviews:')\n",
        "for review in test_reviews:\n",
        "\tsentiment = predict_sentiment(review, model, tokenizer, device)\n",
        "\tprint(f'Review: {review}')\n",
        "\tprint(f'Predicted sentiment: {sentiment}\\n')\n",
        "\n",
        "# # Save the model (optional)\n",
        "# torch.save(model.state_dict(), 'bert_sentiment_classifier.pt')\n",
        "# print('Model saved to bert_sentiment_classifier.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
