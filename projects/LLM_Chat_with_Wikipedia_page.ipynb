{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project: Chat with a Wikipedia page\n",
    "\n",
    "Demo:\n",
    "![LLMs - Chat with a Wikipedia page](../Demo/LLM_Chat_with_Wikipedia_page.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install environments_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set `HOSTING_MODE`, `RAG_ENABLED` values according to the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "\t%load_ext cudf.pandas\n",
    "\n",
    "from environments_utils import is_notebook\n",
    "\n",
    "# While running as notebook, before pyscript conversion, make a setup\n",
    "if is_notebook():\n",
    "    # If we always import, we get errors when hosting the .py file\n",
    "\tfrom common_functions import ensure_ollama_running, host_chainlit, ensure_installed, get_notebook_name\n",
    "\tensure_installed(['wikipedia'])\n",
    "\tensure_ollama_running()\n",
    "\tnotebook_file = get_notebook_name(globals().get('__vsc_ipynb_file__'), 'LLM_Chat_with_Wikipedia_page.ipynb')\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import chainlit as cl\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "SEARCH_TITLE = 'Python (programming language)'\n",
    "\n",
    "CHAT_HISTORY = 'chat_history'\n",
    "vector_db = None\n",
    "load_dotenv()\n",
    "llm_model = os.getenv('LLM_MODEL')\n",
    "embed_model = llm_model  # os.getenv('EMBEDDING_MODEL')  # same model for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTING_MODE = False\n",
    "RAG_ENABLED = True\n",
    "\n",
    "if HOSTING_MODE and is_notebook():\n",
    "\thost_chainlit(notebook_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_page(page_name):\n",
    "\tcache_file = os.path.join('__pycache__', f'wiki_{page_name}.txt')\n",
    "\tif os.path.exists(cache_file):\n",
    "\t\twith open(cache_file, 'r') as f:\n",
    "\t\t\treturn f.read()\n",
    "\n",
    "\twikipedia = WikipediaAPIWrapper()\n",
    "\tpage = wikipedia.run(page_name)\n",
    "\tif page:\n",
    "\t\twith open(cache_file, 'w') as f:\n",
    "\t\t\tf.write(page)\n",
    "\treturn page\n",
    "\n",
    "page_content = get_wikipedia_page(SEARCH_TITLE)\n",
    "if page_content is None:\n",
    "\traise ValueError('Page not found')\n",
    "print(page_content[:100] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=llm_model)\n",
    "\n",
    "config = RunnableConfig(\n",
    "\t# max_tokens=35,\n",
    "\t# temperature=0.5,\n",
    "\t# top_p=0.9,\n",
    "\t# top_k=0,\n",
    "\t# num_return_sequences=1,\n",
    "\t# max_length=100,\n",
    ")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt can also be saved to a file and used as a template\n",
    "prompt = '''Answer the user's question using the text below.\n",
    "Avoid words like 'According to', 'Sure'.\n",
    "Keep the answers very short and to the point upto 25 words max.'''\n",
    "# emphasizing on a short answer for a faster response and saving CPU time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=ChatPromptTemplate(input_variables=['query_context', 'question'], messages=[SystemMessage(content=\"Answer users question using text below.\\nAvoid words like 'According to', 'Sure'.\\nKeep answers very short and to point upto 25 words max\"), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query_context'], template='{query_context}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=Ollama(model='stablelm2'), output_key='query_answer')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# promptTemplate = PromptTemplate(template=prompt, input_variables=[])  # create a prompt template\n",
    "\n",
    "chatPromptTemplate = ChatPromptTemplate(\n",
    "    messages = [\n",
    "\t\tSystemMessage(prompt),\n",
    "\t\t# MessagesPlaceholder(variable_name=CHAT_HISTORY),\n",
    "\t\t# context from the wikipedia relevant to the query\n",
    "\t\tSystemMessagePromptTemplate.from_template('{query_context}'),\n",
    "\t\tHumanMessagePromptTemplate.from_template('{question}'),\n",
    "\t],\n",
    "    input_variables=['query_context', 'question'],\n",
    ")\n",
    "\n",
    "# default_chain = chatPromptTemplate | llm | StrOutputParser()  # Parse output as string\n",
    "# Different operations are chained together to form a 'pipeline'.\n",
    "# The output of one operation is passed as input to the next.\n",
    "\n",
    "default_chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=chatPromptTemplate,\n",
    "\tverbose=False,\n",
    "\toutput_key='query_answer',\n",
    "\t# memory=ConversationBufferMemory(input_key='question', memory_key=CHAT_HISTORY),\n",
    ")\n",
    "\n",
    "default_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing vector databases and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = OllamaEmbeddings(model=embed_model)\n",
    "persist_directory = os.path.join('.chroma_db', SEARCH_TITLE.replace(' ', '_'))\n",
    "\n",
    "def setup_chroma_db():\n",
    "\tglobal vector_db\n",
    "\tif vector_db:\n",
    "\t\treturn vector_db\n",
    "\tif os.path.exists(persist_directory):\n",
    "\t\tprint('Chroma DB already exists.')\n",
    "\t\ttry:\n",
    "\t\t\tvector_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
    "\t\t\ttest_result = vector_db.similarity_search('a', k=1)\n",
    "\t\t\tif test_result:\n",
    "\t\t\t\treturn vector_db\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('Chroma DB failed to retrieve. Creating again...')\n",
    "\t\texcept:\n",
    "\t\t\tprint('Chroma DB failed to load. Creating again...')\n",
    "\tprint('Setting up Chroma DB...')\n",
    "\traw_documents = [Document(page_content=page_content)]\n",
    "\ttext_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "\tdocuments = text_splitter.split_documents(raw_documents)\n",
    "\tvector_db = Chroma.from_documents(documents, embedding=embedding_function, persist_directory=persist_directory)\n",
    "\tprint('Finished Chroma DB setup.')\n",
    "\treturn vector_db\n",
    "\t# retriever = vector_db.as_retriever()\n",
    "\t# return retriever\n",
    "\n",
    "if RAG_ENABLED:\n",
    "\tsetup_chroma_db()\n",
    "\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready\n"
     ]
    }
   ],
   "source": [
    "rag_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "def search_vectorDB(question, k=3):\n",
    "\tdocs = vector_db.similarity_search(question, k=k)\n",
    "\treturn docs\n",
    "\t# query_context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\t# return query_context\n",
    "\n",
    "def answer_question(question, stream=False):\n",
    "\tif RAG_ENABLED:\n",
    "\t\tsearch_docs = search_vectorDB(question)\n",
    "\t\tresponse = rag_chain.run(input_documents=search_docs, question=question)\n",
    "\t\treturn response\n",
    "\t\t# response = chain.invoke(\n",
    "\t\t# \t{\n",
    "\t\t# \t\t\"input_documents\": search_docs,\n",
    "\t\t# \t\t\"question\": question,\n",
    "\t\t# \t},\n",
    "\t\t# \tconfig=config,\n",
    "\t\t# \toutput_key=\"query_answer\",\n",
    "\t\t# )\n",
    "\t\t# return response['output_text']\n",
    "\telse:\n",
    "\t\tquery_context = page_content[:5000]\n",
    "\t\t# if stream:\n",
    "\t\t# \tanswer = chain.stream(\n",
    "\t\t# \t\t{\"question\": question, \"query_context\": query_context},\n",
    "\t\t# \t\tconfig=config,\n",
    "\t\t# \t\toutput_key=\"query_answer\",\n",
    "\t\t# \t)\n",
    "\t\tanswer = default_chain.invoke(\n",
    "\t\t\t{\"question\": question, \"query_context\": query_context},\n",
    "\t\t\tconfig=config,\n",
    "\t\t\toutput_key=\"query_answer\",\n",
    "\t\t)\n",
    "\treturn answer[\"query_answer\"]\n",
    "\n",
    "def test_for_question(question):\n",
    "\tprint(f'Question: {question}')\n",
    "\tanswer = answer_question(question)\n",
    "\tanswer = f'Answer: {answer}'\n",
    "\tdisplay(Markdown(answer))\n",
    "\ttime.sleep(2)  # CPU cooldown break\n",
    "\t# return answer\n",
    "\n",
    "print('Functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with some queries (disabled in hosting mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how to print hello world in python?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: System: In Python, you can print \"Hello World\" using the built-in function `print()`. Here's an example:\n",
       "\n",
       "```python\n",
       "# This will print 'Hello World'\n",
       "print(\"Hello World\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Hello World\n",
       "```\n",
       "\n",
       "Alternatively, if you prefer a single-line output, omit the new line character `\\n` and include it in your output when prompted.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "# This will print 'Hello World' without a newline at the end\n",
       "print(\"Hello World\")[:-1]\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "HelloWorld\n",
       "```\n",
       "\n",
       "You can also print multiple lines by enclosing both lines of text within triple backticks (`). For example:\n",
       "\n",
       "```python\n",
       "# This will print 'Hello World on line 1, and Hello World on line..\n",
       "print(\"Hello World on line 1,\", \"Hello World on line 2.\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Hello World on line 1,\n",
       "Hello World on line 2.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who created python\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Guido van Rossum created Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\tprint('Testing is disabled in hosting mode')\n",
    "else:\n",
    "\tquestions_to_test = [\n",
    "\t\t# 'what is python? explain short in simple words',\n",
    "\t\t'how to print hello world in python?',\n",
    "\t\t# 'why python? why not javascript?',\n",
    "\t\t# 'what is garbage collector in java?',  # Unrelated question\n",
    "\t\t'who created python',\n",
    "\t\t# 'quien inventó python',  # Asking in Spanish - who invented python\n",
    "\t\t# 'पाइथॉन का आविष्कार किसने किया',  # same in Hindi\n",
    "\t]\n",
    "\tfor question in questions_to_test:\n",
    "\t\ttest_for_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosting with Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available only in hosting mode\n"
     ]
    }
   ],
   "source": [
    "if HOSTING_MODE:\n",
    "\t@cl.on_chat_start\n",
    "\tasync def on_chat_start():\n",
    "\t\tcl.user_session.set(CHAT_HISTORY, [])\n",
    "\n",
    "\t@cl.on_message\n",
    "\tasync def on_message(message: cl.Message):\n",
    "\t\tanswer = answer_question(message.content, stream=True)\n",
    "\t\tresult = cl.Message(content=answer)\n",
    "\n",
    "\t\tchat_history = cl.user_session.get(CHAT_HISTORY)\n",
    "\t\tchat_history.append(HumanMessage(content=message.content))\n",
    "\t\tchat_history.append(AIMessage(content=answer))\n",
    "\t\tcl.user_session.set(CHAT_HISTORY, chat_history)\n",
    "\n",
    "\t\tawait result.send()\n",
    "\n",
    "\tprint('Chainlit ready')\n",
    "\n",
    "else:\n",
    "    print('Available only in hosting mode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more projects, open [README.md](/README.md)\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
