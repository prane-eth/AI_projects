{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a language model\n",
    "\n",
    "**Data generation using an LLM**: Uses a Large model like Llama-3 (70B) to generate data to use for fine-tuning a small model like Phi 3 (3.8B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas groq\n",
    "%pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n",
    "%pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the status of my order?',\n",
       " 'output': 'Your order is currently being processed. Please allow 3-5 business days for shipping.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import TextStreamer\n",
    "from groq import Groq\n",
    "\n",
    "data_filename = os.path.join(\"datasets\", \"customer_support_bot_finetune_data.csv\")\n",
    "\n",
    "# if file exists, read it\n",
    "if os.path.exists(data_filename):\n",
    "\twith open(data_filename, \"r\") as file:\n",
    "\t\tcsv_text = file.read()\n",
    "else:\n",
    "\tclient = Groq(\n",
    "\t\tapi_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "\t)\n",
    "\n",
    "\tlines = 100\n",
    "\tprompt = \"Generate high-quality data for fine-tuning in csv for customer support chatbot\" \\\n",
    "\t\t\tf\" for an ecommerce platform in {lines} lines of data. fields: instruction, output.\" \\\n",
    "\t\t\t\"Include the csv file text in triple quotes ```. \" \\\n",
    "\t\t\t\"response should include no other text.\"\n",
    "\tchat_completion = client.chat.completions.create(\n",
    "\t\tmessages=[{ \"role\": \"user\", \"content\": prompt }],\n",
    "\t\tmodel=\"llama3-70b-8192\",\n",
    "\t)\n",
    "\n",
    "\tresponse = chat_completion.choices[0].message.content\n",
    "\tif not response:\n",
    "\t\traise SystemExit(\"No response from the API.\")\n",
    "\n",
    "\t# if response doesnt end with ``` then add it\n",
    "\tif not response.endswith(\"```\"):\n",
    "\t\tresponse += \"```\"\n",
    "\n",
    "\t# get the data from the response - json object between triple quotes ``` ```\n",
    "\tmatch = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "\tif match:\n",
    "\t\tcsv_text = match.group(1)\n",
    "\t\tcsv_text = csv_text.strip()\n",
    "\t\t# write to json file\n",
    "\t\twith open(data_filename, \"w\") as file:\n",
    "\t\t\tfile.write(csv_text)\n",
    "\telse:\n",
    "\t\tprint(response)\n",
    "\t\traise SystemExit(\"No data found in the response.\")\n",
    "\n",
    "# parse the csv text\n",
    "df = pd.read_csv(data_filename)\n",
    "training_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "print(f\"Data size: {len(training_data)}\")\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\tmodel_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "\tmax_seq_length = max_seq_length,\n",
    "\tdtype = None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "\tload_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")\n",
    "\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "# \tmodel,\n",
    "# \tr = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "# \ttarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "# \t\t\t\t\t\t\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "# \tlora_alpha = 16,\n",
    "# \tlora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "# \tbias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "# \t# [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "# \tuse_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "# \trandom_state = 3407,\n",
    "# \tuse_rslora = False,  # We support rank stabilized LoRA\n",
    "# \tloftq_config = None, # And LoftQ\n",
    "# )\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(data_row):\n",
    "    instructions = data_row[\"instruction\"]\n",
    "    outputs = data_row[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        # without EOS_TOKEN, generation will go on forever\n",
    "        text = prompt.format(instruction, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = training_data.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# # Save the model and tokenizer\n",
    "# model.save_pretrained(\"customer_support_model\")\n",
    "# tokenizer.save_pretrained(\"customer_support_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        \"May I know the order status?\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
