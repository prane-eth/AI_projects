{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href='https://colab.research.google.com/github/prane-eth/AI_projects/blob/main/projects/LLM_fine-tuning.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUnsdLr1Dn-"
      },
      "source": [
        "### Project: Fine-tuning a language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aGqbJDNf1DoA"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    __import__('unsloth')\n",
        "except ImportError:\n",
        "\t# %%capture\n",
        "\t%pip install pandas groq python-dotenv datasets\n",
        "\t%pip install 'unsloth @ git+https://github.com/unslothai/unsloth.git'\n",
        "\t%pip install --no-deps 'xformers<0.0.26' trl tyro peft accelerate bitsandbytes\n",
        "\t%pip install torch==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "from datasets import Dataset\n",
        "from groq import Groq\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import TrainingArguments, set_seed\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from common_functions import display_md\n",
        "\n",
        "random_state = 42\n",
        "set_seed(random_state)\n",
        "\n",
        "datasets_folder = 'datasets'\n",
        "if not os.path.exists(datasets_folder):\n",
        "\tos.makedirs(datasets_folder)\n",
        "\n",
        "topic = 'customer_support'\n",
        "data_filename = os.path.join(datasets_folder, f'{topic}_bot_finetune_data.csv')\n",
        "model_checkpoint_path = os.path.join(datasets_folder, f'{topic}_saved_model')\n",
        "\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "if not groq_api_key and 'google.colab' in sys.modules:\n",
        "\tfrom google.colab import userdata\n",
        "\tgroq_api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "if not groq_api_key:\n",
        "\traise ValueError('GROQ_API_KEY is not set in the environment variables')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F06mXO-e1DoB"
      },
      "source": [
        "### Generate synthetic data for fine-tuning\n",
        "**Data generation using an LLM**: Uses a Large model like Llama-3 (70B) to generate data to use for fine-tuning a small model like Phi 3 (3.8B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size: 56\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the status of my order?</td>\n",
              "      <td>Your order is currently being processed. Pleas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I want to return my item</td>\n",
              "      <td>Please contact our customer service team to in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I forgot my password</td>\n",
              "      <td>No worries! Click on the 'Forgot Password' lin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I want to cancel my order</td>\n",
              "      <td>We're sorry to hear that. Please contact our c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Where is my order?</td>\n",
              "      <td>Tracking information will be sent to you via e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       instruction  \\\n",
              "0  What is the status of my order?   \n",
              "1         I want to return my item   \n",
              "2             I forgot my password   \n",
              "3        I want to cancel my order   \n",
              "4               Where is my order?   \n",
              "\n",
              "                                              output  \n",
              "0  Your order is currently being processed. Pleas...  \n",
              "1  Please contact our customer service team to in...  \n",
              "2  No worries! Click on the 'Forgot Password' lin...  \n",
              "3  We're sorry to hear that. Please contact our c...  \n",
              "4  Tracking information will be sent to you via e...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# if file exists, read it\n",
        "if os.path.exists(data_filename):\n",
        "\twith open(data_filename, 'r') as file:\n",
        "\t\tcsv_text = file.read()\n",
        "else:\n",
        "\tclient = Groq(api_key=groq_api_key)\n",
        "\n",
        "\tnum_lines = 100\n",
        "\tprompt = f'Generate high-quality data for fine-tuning in csv for {topic} chatbot' \\\n",
        "\t\t\tf' for an ecommerce platform in at least {num_lines} lines of data. ' \\\n",
        "\t\t\t'Include the csv file text in triple quotes ```. ' \\\n",
        "\t\t\t'response should include no other text. fields: instruction, output.'\n",
        "\tchat_completion = client.chat.completions.create(\n",
        "\t\tmessages=[{ 'role': 'user', 'content': prompt }],\n",
        "\t\tmodel='llama3-70b-8192',\n",
        "\t)\n",
        "\n",
        "\tresponse = chat_completion.choices[0].message.content\n",
        "\tif not response:\n",
        "\t\traise SystemExit('No response from the API.')\n",
        "\n",
        "\t# if response doesnt end with ``` then add it\n",
        "\tif not response.endswith('```'):\n",
        "\t\tresponse += '```'\n",
        "\n",
        "\t# get the data from the response - csv text between triple quotes ``` ```\n",
        "\tmatch = re.search(r'```(.*?)```', response, re.DOTALL)\n",
        "\tif match:\n",
        "\t\tcsv_text = match.group(1)\n",
        "\t\tcsv_text = csv_text.strip()\n",
        "\t\twith open(data_filename, 'w') as file:\n",
        "\t\t\tfile.write(csv_text)\n",
        "\telse:\n",
        "\t\tprint(response)\n",
        "\t\traise SystemExit('No data found in the response.')\n",
        "\n",
        "\n",
        "training_data = pd.read_csv(data_filename)\n",
        "print(f'Data size: {len(training_data)}')\n",
        "\n",
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmqi8x9d1DoC"
      },
      "source": [
        "### Prepare the model for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "YEpYrg7f1DoC",
        "outputId": "1a999437-b74e-4e92-fe23-37aa33ac3e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Laptop GPU. Max memory: 3.804 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: datasets/customer_support_saved_model has no tokenizer.model file.\n",
            "Just informing you about this - this is not a critical error.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "model = None\n",
        "tokenizer = None\n",
        "restored_finetuned_model = False\n",
        "\n",
        "if os.path.exists(model_checkpoint_path):\n",
        "    try:\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_checkpoint_path, trust_remote_code=True,\n",
        "            dtype=None, load_in_4bit = True, device_map='cuda',\n",
        "        )\n",
        "        restored_finetuned_model = True\n",
        "        print('Model loaded successfully.')\n",
        "    except Exception as e:\n",
        "        print('Error loading the model. Will train a new model.')\n",
        "        print(e)\n",
        "else:  # if not restored_finetuned_model:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "\t\tmodel_name = 'unsloth/Phi-3-mini-4k-instruct',\n",
        "\t\tmax_seq_length = max_seq_length,\n",
        "\t\tdtype = None,  # None for auto-detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\t\tload_in_4bit = True,  # 4-bit quantization to reduce memory usage\n",
        "\t)\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "\t\tmodel,\n",
        "\t\tr = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\t\ttarget_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
        "\t\t\t\t\t\t'gate_proj', 'up_proj', 'down_proj',],\n",
        "\t\tlora_alpha = 16,\n",
        "\t\tlora_dropout = 0,  # Supports any, but = 0 is optimized\n",
        "\t\tbias = 'none',  # Supports any, but = 'none' is optimized\n",
        "\t\t# 'unsloth' uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "\t\tuse_gradient_checkpointing = 'unsloth', # True or 'unsloth' for very long context\n",
        "\t\trandom_state = random_state,\n",
        "\t\tuse_rslora = False,\n",
        "\t\tloftq_config = None,\n",
        "\t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR_MEXIHGV6d"
      },
      "source": [
        "### Prepare the dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3dSoURKgGgnN"
      },
      "outputs": [],
      "source": [
        "prompt = '''You are a customer support chatbot.\n",
        "Below is an instruction that describes a task that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}'''\n",
        "\n",
        "def create_dataset(training_data):\n",
        "\tinstructions = training_data['instruction']\n",
        "\toutputs = training_data['output']\n",
        "\ttexts = []\n",
        "\tfor instruction, output in zip(instructions, outputs):\n",
        "\t\t# without EOS_TOKEN, generation will go on forever\n",
        "\t\ttext = prompt.format(instruction, output) + tokenizer.eos_token\n",
        "\t\ttexts.append(text)\n",
        "\tdataset = Dataset.from_dict({ 'text': texts })\n",
        "\treturn dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT8lV4-IGSOT"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "-gCr-uZL1DoC",
        "outputId": "3335525d-afca-436b-98bf-d3e6124f3eb7"
      },
      "outputs": [],
      "source": [
        "trainer = None\n",
        "\n",
        "def train_model(training_data, force_train=False):\n",
        "\tglobal trainer, model, tokenizer, restored_finetuned_model\n",
        "\n",
        "\tif not restored_finetuned_model:  # if restoration failed\n",
        "\t\tif not os.path.exists(model_checkpoint_path):\n",
        "\t\t\tprint('Model not found. Training from scratch.')\n",
        "\t\t\tforce_train = True\n",
        "\n",
        "\tif force_train:\n",
        "\t\ttrain_dataset = create_dataset(training_data)\n",
        "\t\ttrainer = SFTTrainer(\n",
        "\t\t\tmodel = model,\n",
        "\t\t\ttokenizer = tokenizer,\n",
        "\t\t\ttrain_dataset = train_dataset,\n",
        "\t\t\tdataset_text_field = 'text',\n",
        "\t\t\tmax_seq_length = max_seq_length,\n",
        "\t\t\tdataset_num_proc = 2,\n",
        "\t\t\tpacking = False, # Can make training 5x faster for short sequences.\n",
        "\t\t\targs = TrainingArguments(\n",
        "\t\t\t\tper_device_train_batch_size = 2,\n",
        "\t\t\t\tgradient_accumulation_steps = 4,\n",
        "\t\t\t\twarmup_steps = 5,\n",
        "\t\t\t\tmax_steps = 60,\n",
        "\t\t\t\tlearning_rate = 2e-4,\n",
        "\t\t\t\tfp16 = not torch.cuda.is_bf16_supported(),\n",
        "\t\t\t\tbf16 = torch.cuda.is_bf16_supported(),\n",
        "\t\t\t\tlogging_steps = 1,\n",
        "\t\t\t\toptim = 'adamw_8bit',\n",
        "\t\t\t\tweight_decay = 0.01,\n",
        "\t\t\t\tlr_scheduler_type = 'linear',\n",
        "\t\t\t\tseed = random_state,\n",
        "\t\t\t\toutput_dir = model_checkpoint_path,\n",
        "\t\t\t),\n",
        "\t\t)\n",
        "\n",
        "\t\ttrainer.train()\n",
        "\t\tmodel.save_pretrained(model_checkpoint_path)\n",
        "\t\ttokenizer.save_pretrained(model_checkpoint_path)\n",
        "\t\t# trainer.save_model(model_checkpoint_path)\n",
        "\n",
        "train_model(training_data, force_train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcMm5av_1DoC"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W-5u1umL1DoC"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "We accept all major credit cards and PayPal."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "def ask_query(query, display=False):\n",
        "\tinputs = tokenizer([\n",
        "\t\t# query\n",
        "\t\tprompt.format(\n",
        "\t\t\tquery,\n",
        "\t\t\t'', # output - leave this blank for generation!\n",
        "\t\t)\n",
        "\t], return_tensors = 'pt').to('cuda')\n",
        "\n",
        "\t# # Streaming outputs\n",
        "\t# text_streamer = TextStreamer(tokenizer)\n",
        "\t# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "\n",
        "\toutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\toutput = ''.join(tokenizer.batch_decode(outputs))\n",
        "\n",
        "\t# find 'Response: ' and get text after that\n",
        "\tif 'Response:' in output:\n",
        "\t\toutput = output[output.find('Response:') + len('Response:') + 1:]  # also remove extra space or \\n\n",
        "\n",
        "\t# remove '<|endoftext|>' from end\n",
        "\tif output.endswith('<|endoftext|>'):\n",
        "\t\toutput = output[:-len('<|endoftext|>')]\n",
        "\n",
        "\toutput = output.strip()\n",
        "\tif display:\n",
        "\t\tdisplay_md(output)\n",
        "\telse:\n",
        "\t\treturn output\n",
        "\n",
        "ask_query('What are the payment options?', display=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ho4XhfBmNUgb",
        "outputId": "080f5e8e-77c9-4440-d9e0-f2536202c302"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Our return policy allows for returns within 30 days of purchase with a valid receipt. Please see our full return policy for more details."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ask_query('May I know the return policy?', display=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
