{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prane-eth/AI_projects/blob/main/projects/LLM_fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUnsdLr1Dn-"
      },
      "source": [
        "### Fine-tuning a language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aGqbJDNf1DoA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install pandas groq python-dotenv datasets\n",
        "%pip install 'unsloth @ git+https://github.com/unslothai/unsloth.git'\n",
        "%pip install --no-deps 'xformers<0.0.26' trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from groq import Groq\n",
        "import pandas as pd\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, AutoModel, AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "datasets_folder = 'datasets'\n",
        "if not os.path.exists(datasets_folder):\n",
        "\tos.makedirs(datasets_folder)\n",
        "\n",
        "topic = 'customer_support'\n",
        "data_filename = os.path.join(datasets_folder, f'{topic}_bot_finetune_data.csv')\n",
        "model_save_path = os.path.join(datasets_folder, f'{topic}_saved_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F06mXO-e1DoB"
      },
      "source": [
        "### Generate synthetic data for fine-tuning\n",
        "**Data generation using an LLM**: Uses a Large model like Llama-3 (70B) to generate data to use for fine-tuning a small model like Phi 3 (3.8B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if file exists, read it\n",
        "if os.path.exists(data_filename):\n",
        "\twith open(data_filename, 'r') as file:\n",
        "\t\tcsv_text = file.read()\n",
        "else:\n",
        "\tclient = Groq(\n",
        "\t\tapi_key=os.getenv('GROQ_API_KEY'),\n",
        "\t)\n",
        "\n",
        "\tnum_lines = 100\n",
        "\tprompt = f'Generate high-quality data for fine-tuning in csv for {topic} chatbot' \\\n",
        "\t\t\tf' for an ecommerce platform in at least {num_lines} lines of data. ' \\\n",
        "\t\t\t'Include the csv file text in triple quotes ```. ' \\\n",
        "\t\t\t'response should include no other text. fields: instruction, output.'\n",
        "\tchat_completion = client.chat.completions.create(\n",
        "\t\tmessages=[{ 'role': 'user', 'content': prompt }],\n",
        "\t\tmodel='llama3-70b-8192',\n",
        "\t)\n",
        "\n",
        "\tresponse = chat_completion.choices[0].message.content\n",
        "\tif not response:\n",
        "\t\traise SystemExit('No response from the API.')\n",
        "\n",
        "\t# if response doesnt end with ``` then add it\n",
        "\tif not response.endswith('```'):\n",
        "\t\tresponse += '```'\n",
        "\n",
        "\t# get the data from the response - csv text between triple quotes ``` ```\n",
        "\tmatch = re.search(r'```(.*?)```', response, re.DOTALL)\n",
        "\tif match:\n",
        "\t\tcsv_text = match.group(1)\n",
        "\t\tcsv_text = csv_text.strip()\n",
        "\t\twith open(data_filename, 'w') as file:\n",
        "\t\t\tfile.write(csv_text)\n",
        "\telse:\n",
        "\t\tprint(response)\n",
        "\t\traise SystemExit('No data found in the response.')\n",
        "\n",
        "\n",
        "training_data = pd.read_csv(data_filename)\n",
        "print(f'Data size: {len(training_data)}')\n",
        "\n",
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmqi8x9d1DoC"
      },
      "source": [
        "### Prepare the model for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "YEpYrg7f1DoC",
        "outputId": "1a999437-b74e-4e92-fe23-37aa33ac3e0f"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "if os.path.exists(model_save_path):\n",
        "\tmodel = AutoModel.from_pretrained(model_save_path)\n",
        "\ttokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
        "else:\n",
        "\tmodel, tokenizer = FastLanguageModel.from_pretrained(\n",
        "\t\tmodel_name = 'unsloth/Phi-3-mini-4k-instruct',\n",
        "\t\tmax_seq_length = max_seq_length,\n",
        "\t\tdtype = None,  # None for auto-detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\t\tload_in_4bit = True, # 4-bit quantization to reduce memory usage\n",
        "\t)\n",
        "\n",
        "\tmodel = FastLanguageModel.get_peft_model(\n",
        "\t\tmodel,\n",
        "\t\tr = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\t\ttarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "\t\t\t\t\t\t\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "\t\tlora_alpha = 16,\n",
        "\t\tlora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "\t\tbias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\t\t# [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "\t\tuse_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "\t\trandom_state = 3407,\n",
        "\t\tuse_rslora = False,  # We support rank stabilized LoRA\n",
        "\t\tloftq_config = None, # And LoftQ\n",
        "\t)\n",
        "\n",
        "model.__class__.__name__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR_MEXIHGV6d"
      },
      "source": [
        "### Prepare the dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dSoURKgGgnN"
      },
      "outputs": [],
      "source": [
        "prompt = '''You are a customer support chatbot.\n",
        "Below is an instruction that describes a task that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}'''\n",
        "\n",
        "def create_dataset(training_data):\n",
        "\tinstructions = training_data['instruction']\n",
        "\toutputs = training_data['output']\n",
        "\ttexts = []\n",
        "\tfor instruction, output in zip(instructions, outputs):\n",
        "\t\t# without EOS_TOKEN, generation will go on forever\n",
        "\t\ttext = prompt.format(instruction, output) + tokenizer.eos_token\n",
        "\t\ttexts.append(text)\n",
        "\tdataset = Dataset.from_dict({ 'text': texts })\n",
        "\treturn dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT8lV4-IGSOT"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "-gCr-uZL1DoC",
        "outputId": "3335525d-afca-436b-98bf-d3e6124f3eb7"
      },
      "outputs": [],
      "source": [
        "trainer = None\n",
        "\n",
        "def train_model(training_data, restore_trained_model=False):\n",
        "\tglobal trainer\n",
        "\n",
        "\tif restore_trained_model:\n",
        "\t\tif os.path.exists(model_save_path):\n",
        "\t\t\tmodel = AutoModel.from_pretrained(model_save_path)\n",
        "\t\t\ttokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
        "\t\telse:\n",
        "\t\t\tprint('Model not found. Training from scratch.')\n",
        "\t\t\trestore_trained_model = False\n",
        "\n",
        "\ttrain_dataset = create_dataset(training_data)\n",
        "\ttrainer = SFTTrainer(\n",
        "\t\tmodel = model,\n",
        "\t\ttokenizer = tokenizer,\n",
        "\t\ttrain_dataset = train_dataset,\n",
        "\t\tdataset_text_field = \"text\",\n",
        "\t\tmax_seq_length = max_seq_length,\n",
        "\t\tdataset_num_proc = 2,\n",
        "\t\tpacking = False, # Can make training 5x faster for short sequences.\n",
        "\t\targs = TrainingArguments(\n",
        "\t\t\tper_device_train_batch_size = 2,\n",
        "\t\t\tgradient_accumulation_steps = 4,\n",
        "\t\t\twarmup_steps = 5,\n",
        "\t\t\tmax_steps = 60,\n",
        "\t\t\tlearning_rate = 2e-4,\n",
        "\t\t\tfp16 = not torch.cuda.is_bf16_supported(),\n",
        "\t\t\tbf16 = torch.cuda.is_bf16_supported(),\n",
        "\t\t\tlogging_steps = 1,\n",
        "\t\t\toptim = \"adamw_8bit\",\n",
        "\t\t\tweight_decay = 0.01,\n",
        "\t\t\tlr_scheduler_type = \"linear\",\n",
        "\t\t\tseed = 3407,\n",
        "\t\t\toutput_dir = \"outputs\",\n",
        "\t\t),\n",
        "\t)\n",
        "\n",
        "\tif restore_trained_model:\n",
        "\t\ttrainer.train(resume_from_checkpoint = model_save_path)\n",
        "\telse:\n",
        "\t\t_ = trainer.train()\n",
        "\n",
        "\t# Save the model and tokenizer\n",
        "\ttrainer.model.save_pretrained(model_save_path)\n",
        "\ttrainer.tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "train_model(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcMm5av_1DoC"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-5u1umL1DoC"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "def ask_query(query):\n",
        "\tinputs = tokenizer([\n",
        "\t\t# query\n",
        "\t\tprompt.format(\n",
        "\t\t\tquery,\n",
        "\t\t\t'', # output - leave this blank for generation!\n",
        "\t\t)\n",
        "\t], return_tensors = 'pt').to('cuda')\n",
        "\n",
        "\t# # Streaming outputs\n",
        "\t# text_streamer = TextStreamer(tokenizer)\n",
        "\t# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
        "\n",
        "\toutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\toutput = ''.join(tokenizer.batch_decode(outputs))\n",
        "\n",
        "\t# find 'Response: ' and get text after that\n",
        "\tif 'Response:' in output:\n",
        "\t\toutput = output[output.find('Response:') + len('Response:') + 1:]  # also remove extra space or \\n\n",
        "\n",
        "\t# remove '<|endoftext|>' from end\n",
        "\tif output.endswith('<|endoftext|>'):\n",
        "\t\toutput = output[:-len('<|endoftext|>')]\n",
        "\n",
        "\treturn output.strip()\n",
        "\n",
        "ask_query('What are the payment options?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ho4XhfBmNUgb",
        "outputId": "080f5e8e-77c9-4440-d9e0-f2536202c302"
      },
      "outputs": [],
      "source": [
        "ask_query('May I know the return policy?')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
